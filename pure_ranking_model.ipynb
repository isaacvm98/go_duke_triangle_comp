{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e28c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "zip_path = \"march-machine-learning-mania-2025.zip\"\n",
    "inner_file = \"MRegularSeasonDetailedResults.csv\"\n",
    "conference_file = \"MTeamConferences.csv\"\n",
    "mmassey_rankings = 'MMasseyOrdinals.csv'\n",
    "\n",
    "with zipfile.ZipFile(zip_path) as z:\n",
    "    with z.open(inner_file) as f1, z.open(conference_file) as f2, z.open(mmassey_rankings) as f3:\n",
    "        df_teams = pd.read_csv(f1)\n",
    "        df_conferences = pd.read_csv(f2)\n",
    "        df_massey = pd.read_csv(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc58beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CONFERENCES = ['acc', 'big_twelve', 'sec', 'big_ten', 'pac_twelve', 'big_east']\n",
    "teams_in_target_confs = df_conferences[\n",
    "    df_conferences['ConfAbbrev'].isin(TARGET_CONFERENCES)\n",
    "]['TeamID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9910eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_point_spread(df_teams):\n",
    "    \"\"\"\n",
    "    Calculate home team point spread correctly\n",
    "    WLoc refers to the WINNING team's location:\n",
    "    - 'H': Winner was home team\n",
    "    - 'A': Winner was away team  \n",
    "    - 'N': Neutral site\n",
    "    \"\"\"\n",
    "    df = df_teams.copy()\n",
    "    \n",
    "    # Determine home and away teams based on WLoc\n",
    "    # If WLoc == 'H': WTeamID is home, LTeamID is away\n",
    "    # If WLoc == 'A': LTeamID is home, WTeamID is away\n",
    "    # If WLoc == 'N': Neutral site (we'll use WTeamID as \"team1\", LTeamID as \"team2\")\n",
    "    \n",
    "    df['HomeTeamID'] = np.where(df['WLoc'] == 'H', df['WTeamID'],\n",
    "                        np.where(df['WLoc'] == 'A', df['LTeamID'],\n",
    "                                df['WTeamID']))  # For neutral, just pick one\n",
    "    \n",
    "    df['AwayTeamID'] = np.where(df['WLoc'] == 'H', df['LTeamID'],\n",
    "                        np.where(df['WLoc'] == 'A', df['WTeamID'],\n",
    "                                df['LTeamID']))\n",
    "    \n",
    "    df['HomeScore'] = np.where(df['WLoc'] == 'H', df['WScore'],\n",
    "                      np.where(df['WLoc'] == 'A', df['LScore'],\n",
    "                              df['WScore']))  # For neutral, just pick one\n",
    "    \n",
    "    df['AwayScore'] = np.where(df['WLoc'] == 'H', df['LScore'],\n",
    "                      np.where(df['WLoc'] == 'A', df['WScore'],\n",
    "                              df['LScore']))\n",
    "    \n",
    "    # Point spread from home team perspective\n",
    "    # Positive = home team won by X points\n",
    "    # Negative = home team lost by X points\n",
    "    df['HomeSpread'] = df['HomeScore'] - df['AwayScore']\n",
    "    \n",
    "    # Mark neutral site games\n",
    "    df['IsNeutral'] = (df['WLoc'] == 'N').astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_games = calculate_point_spread(df_teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6baf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking_for_game(df_games, df_massey, ranking_systems=['POM']):\n",
    "    \"\"\"\n",
    "    For each game on DayNum X, get rankings from the most recent RankingDayNum < X\n",
    "    \n",
    "    This ensures we only use information available BEFORE the game\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter to desired ranking systems\n",
    "    rankings = df_massey[df_massey['SystemName'].isin(ranking_systems)].copy()\n",
    "    \n",
    "    # Sort by season, team, system, and ranking day\n",
    "    rankings = rankings.sort_values(['Season', 'TeamID', 'SystemName', 'RankingDayNum'])\n",
    "    \n",
    "    # Create a lookup structure for faster access\n",
    "    print(\"Creating ranking lookup structure...\")\n",
    "    rankings_dict = {}\n",
    "    for system in ranking_systems:\n",
    "        system_ranks = rankings[rankings['SystemName'] == system]\n",
    "        for season in system_ranks['Season'].unique():\n",
    "            season_ranks = system_ranks[system_ranks['Season'] == season]\n",
    "            if season not in rankings_dict:\n",
    "                rankings_dict[season] = {}\n",
    "            rankings_dict[season][system] = season_ranks\n",
    "    \n",
    "    game_features = []\n",
    "    skipped_games = 0\n",
    "\n",
    "    total_games = len(df_games)\n",
    "    \n",
    "    for idx, (game_idx, game) in enumerate(df_games.iterrows()):\n",
    "        \n",
    "        season = game['Season']\n",
    "        game_day = game['DayNum']\n",
    "        home_team = game['HomeTeamID']\n",
    "        away_team = game['AwayTeamID']\n",
    "        \n",
    "        # Check if we have rankings for this season\n",
    "        if season not in rankings_dict:\n",
    "            skipped_games += 1\n",
    "            continue\n",
    "        \n",
    "        # Initialize features\n",
    "        features = {\n",
    "            'Season': season,\n",
    "            'DayNum': game_day,\n",
    "            'HomeTeamID': home_team,\n",
    "            'AwayTeamID': away_team,\n",
    "            'HomeSpread': game['HomeSpread'],\n",
    "            'IsNeutral': game['IsNeutral']\n",
    "        }\n",
    "        \n",
    "        has_all_rankings = True\n",
    "        \n",
    "        for system in ranking_systems:\n",
    "            # Check if system exists for this season\n",
    "            if system not in rankings_dict[season]:\n",
    "                has_all_rankings = False\n",
    "                break\n",
    "            \n",
    "            system_rankings = rankings_dict[season][system]\n",
    "            \n",
    "            # Get rankings BEFORE game day\n",
    "            available_rankings = system_rankings[system_rankings['RankingDayNum'] < game_day]\n",
    "            \n",
    "            if len(available_rankings) == 0:\n",
    "                has_all_rankings = False\n",
    "                break\n",
    "            \n",
    "            # Home team ranking\n",
    "            home_rankings = available_rankings[available_rankings['TeamID'] == home_team]\n",
    "            if len(home_rankings) > 0:\n",
    "                # Get most recent ranking\n",
    "                latest_home = home_rankings.loc[home_rankings['RankingDayNum'].idxmax()]\n",
    "                features[f'Home_{system}_Rank'] = latest_home['OrdinalRank']\n",
    "                features[f'Home_{system}_RankDay'] = latest_home['RankingDayNum']\n",
    "            else:\n",
    "                has_all_rankings = False\n",
    "                break\n",
    "            \n",
    "            # Away team ranking\n",
    "            away_rankings = available_rankings[available_rankings['TeamID'] == away_team]\n",
    "            if len(away_rankings) > 0:\n",
    "                latest_away = away_rankings.loc[away_rankings['RankingDayNum'].idxmax()]\n",
    "                features[f'Away_{system}_Rank'] = latest_away['OrdinalRank']\n",
    "                features[f'Away_{system}_RankDay'] = latest_away['RankingDayNum']\n",
    "            else:\n",
    "                has_all_rankings = False\n",
    "                break\n",
    "            \n",
    "            # Derived feature: ranking difference\n",
    "            features[f'{system}_RankDiff'] = features[f'Away_{system}_Rank'] - features[f'Home_{system}_Rank']\n",
    "        \n",
    "        # Only add if we have all rankings\n",
    "        if has_all_rankings:\n",
    "            game_features.append(features)\n",
    "        else:\n",
    "            skipped_games += 1\n",
    "    \n",
    "    return pd.DataFrame(game_features)\n",
    "\n",
    "\n",
    "# Use multiple ranking systems\n",
    "ranking_systems = ['POM']\n",
    "\n",
    "# Get features for all games\n",
    "df_model = get_ranking_for_game(df_games, df_massey, ranking_systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e72ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_date_range=(90, 120)\n",
    "training_seasons=range(2010, 2025)\n",
    "training_data = df_model[\n",
    "    (df_model['Season'].isin(training_seasons)) &\n",
    "    (df_model['DayNum'] <= target_date_range[1])  # Use all games up to March\n",
    "].copy()\n",
    "\n",
    "test_data = df_model[\n",
    "    (df_model['Season'] == 2025) &\n",
    "    (df_model['DayNum'] >= target_date_range[0]) &\n",
    "    (df_model['DayNum'] <= target_date_range[1])\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca45cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "def build_baseline_model(training_data, ranking_systems=['POM']):\n",
    "    \"\"\"\n",
    "    Build baseline model using ranking features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define features\n",
    "    feature_cols = []\n",
    "    \n",
    "    for system in ranking_systems:\n",
    "        feature_cols.extend([\n",
    "            f'Home_{system}_Rank',\n",
    "            f'Away_{system}_Rank',\n",
    "            f'{system}_RankDiff'\n",
    "        ])\n",
    "    \n",
    "    # Add neutral site indicator\n",
    "    feature_cols.append('IsNeutral')\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = training_data[feature_cols].copy()\n",
    "    y = training_data['HomeSpread'].copy()\n",
    "    \n",
    "    # Drop rows with missing values\n",
    "    mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    print(f\"\\nTraining samples after removing NaNs: {len(X)}\")\n",
    "    print(f\"Features: {feature_cols}\")\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(X.describe())\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train multiple models and compare performance\n",
    "    \"\"\"\n",
    "    \n",
    "    models = {\n",
    "        'Ridge': Ridge(alpha=1.0),\n",
    "        'Lasso': Lasso(alpha=0.1),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    trained_models = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training {name}...\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[name] = model\n",
    "        \n",
    "        # Predict on train\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_mae = mean_absolute_error(y_train, train_pred)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "        train_r2 = r2_score(y_train, train_pred)\n",
    "        \n",
    "        # Predict on test\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_mae = mean_absolute_error(y_test, test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "        test_r2 = r2_score(y_test, test_pred)\n",
    "        \n",
    "        # Cross-validation on training set\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, \n",
    "                                     scoring='neg_mean_absolute_error')\n",
    "        cv_mae = -cv_scores.mean()\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Train_MAE': train_mae,\n",
    "            'Train_RMSE': train_rmse,\n",
    "            'Train_R2': train_r2,\n",
    "            'Test_MAE': test_mae,\n",
    "            'Test_RMSE': test_rmse,\n",
    "            'Test_R2': test_r2,\n",
    "            'CV_MAE': cv_mae\n",
    "        })\n",
    "        \n",
    "        print(f\"Train MAE: {train_mae:.3f}, RMSE: {train_rmse:.3f}, R²: {train_r2:.3f}\")\n",
    "        print(f\"Test MAE: {test_mae:.3f}, RMSE: {test_rmse:.3f}, R²: {test_r2:.3f}\")\n",
    "        print(f\"CV MAE: {cv_mae:.3f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = results_df.loc[results_df['Test_MAE'].idxmin(), 'Model']\n",
    "    print(f\"\\nBest model by Test MAE: {best_model_name}\")\n",
    "    \n",
    "    return trained_models, results_df\n",
    "\n",
    "X_train, y_train, feature_cols = build_baseline_model(training_data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "trained_models, results_df = train_and_evaluate_models(\n",
    "    X_train_split, y_train_split, X_val_split, y_val_split\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5743a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(trained_models, feature_cols):\n",
    "    \"\"\"\n",
    "    Analyze which features are most important\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get feature importance from tree-based models\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    for idx, (model_name, ax) in enumerate(zip(['Random Forest', 'Gradient Boosting'], axes)):\n",
    "        if model_name in trained_models:\n",
    "            model = trained_models[model_name]\n",
    "            \n",
    "            importances = model.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1][:15]  # Top 15 features\n",
    "            \n",
    "            ax.barh(range(len(indices)), importances[indices], color='steelblue', alpha=0.8)\n",
    "            ax.set_yticks(range(len(indices)))\n",
    "            ax.set_yticklabels([feature_cols[i] for i in indices])\n",
    "            ax.set_xlabel('Feature Importance', fontweight='bold')\n",
    "            ax.set_title(f'{model_name}\\nTop 15 Features', fontweight='bold', fontsize=12)\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions(trained_models, X_val, y_val, model_name='Gradient Boosting'):\n",
    "    \"\"\"\n",
    "    Plot predicted vs actual spreads\n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name not in trained_models:\n",
    "        print(f\"Model {model_name} not found!\")\n",
    "        return\n",
    "    \n",
    "    model = trained_models[model_name]\n",
    "    predictions = model.predict(X_val)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax1 = axes[0]\n",
    "    ax1.scatter(y_val, predictions, alpha=0.5, s=20)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_val.min(), predictions.min())\n",
    "    max_val = max(y_val.max(), predictions.max())\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax1.set_xlabel('Actual Home Spread', fontweight='bold', fontsize=11)\n",
    "    ax1.set_ylabel('Predicted Home Spread', fontweight='bold', fontsize=11)\n",
    "    ax1.set_title(f'{model_name}\\nPredicted vs Actual', fontweight='bold', fontsize=12)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals\n",
    "    ax2 = axes[1]\n",
    "    residuals = y_val - predictions\n",
    "    ax2.scatter(predictions, residuals, alpha=0.5, s=20)\n",
    "    ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    ax2.set_xlabel('Predicted Home Spread', fontweight='bold', fontsize=11)\n",
    "    ax2.set_ylabel('Residual (Actual - Predicted)', fontweight='bold', fontsize=11)\n",
    "    ax2.set_title('Residual Plot', fontweight='bold', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print error distribution\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PREDICTION ERROR ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Mean Absolute Error: {np.abs(residuals).mean():.3f}\")\n",
    "    print(f\"Median Absolute Error: {np.median(np.abs(residuals)):.3f}\")\n",
    "    print(f\"RMSE: {np.sqrt((residuals**2).mean()):.3f}\")\n",
    "    print(f\"\\nError percentiles:\")\n",
    "    print(f\"  25th: {np.percentile(np.abs(residuals), 25):.3f}\")\n",
    "    print(f\"  50th: {np.percentile(np.abs(residuals), 50):.3f}\")\n",
    "    print(f\"  75th: {np.percentile(np.abs(residuals), 75):.3f}\")\n",
    "    print(f\"  90th: {np.percentile(np.abs(residuals), 90):.3f}\")\n",
    "\n",
    "\n",
    "# Run analyses\n",
    "analyze_feature_importance(trained_models, feature_cols)\n",
    "plot_predictions(trained_models, X_val_split, y_val_split, 'Gradient Boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb515103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def create_comprehensive_features(df_model, ranking_systems=['POM', 'MOR', 'DOK', 'MAS', 'SAG']):\n",
    "    \"\"\"\n",
    "    Create multiple representations of rankings with proper handling of edge cases\n",
    "    \"\"\"\n",
    "    df = df_model.copy()\n",
    "    \n",
    "    max_rank = 351\n",
    "    \n",
    "    for system in ranking_systems:\n",
    "        # 1. Keep original ranks (for tree models)\n",
    "        # Already have: Home_{system}_Rank, Away_{system}_Rank, {system}_RankDiff\n",
    "        \n",
    "        # 2. Inverse strength (linear relationship)\n",
    "        # Clip ranks to reasonable range to avoid negatives\n",
    "        home_rank_clipped = np.clip(df[f'Home_{system}_Rank'], 1, max_rank)\n",
    "        away_rank_clipped = np.clip(df[f'Away_{system}_Rank'], 1, max_rank)\n",
    "        \n",
    "        df[f'Home_{system}_Strength'] = max_rank - home_rank_clipped + 1\n",
    "        df[f'Away_{system}_Strength'] = max_rank - away_rank_clipped + 1\n",
    "        df[f'{system}_StrengthDiff'] = df[f'Home_{system}_Strength'] - df[f'Away_{system}_Strength']\n",
    "        \n",
    "        # 3. Log strength (diminishing returns at top)\n",
    "        # Add small epsilon to avoid log(0), use clipped strengths\n",
    "        df[f'Home_{system}_LogStrength'] = np.log(df[f'Home_{system}_Strength'] + 1)\n",
    "        df[f'Away_{system}_LogStrength'] = np.log(df[f'Away_{system}_Strength'] + 1)\n",
    "        \n",
    "        # 4. Squared inverse (emphasize top teams)\n",
    "        df[f'Home_{system}_Strength2'] = df[f'Home_{system}_Strength'] ** 2\n",
    "        df[f'Away_{system}_Strength2'] = df[f'Away_{system}_Strength'] ** 2\n",
    "        \n",
    "        # 5. Boolean indicators for elite teams\n",
    "        df[f'Home_{system}_IsTop25'] = (df[f'Home_{system}_Rank'] <= 25).astype(int)\n",
    "        df[f'Away_{system}_IsTop25'] = (df[f'Away_{system}_Rank'] <= 25).astype(int)\n",
    "        df[f'Home_{system}_IsTop50'] = (df[f'Home_{system}_Rank'] <= 50).astype(int)\n",
    "        df[f'Away_{system}_IsTop50'] = (df[f'Away_{system}_Rank'] <= 50).astype(int)\n",
    "        \n",
    "        # 6. Matchup indicators\n",
    "        df[f'{system}_BothTop25'] = (df[f'Home_{system}_IsTop25'] & df[f'Away_{system}_IsTop25']).astype(int)\n",
    "        df[f'{system}_BothTop50'] = (df[f'Home_{system}_IsTop50'] & df[f'Away_{system}_IsTop50']).astype(int)\n",
    "    \n",
    "    # 7. Consensus features across ranking systems\n",
    "    rank_cols = [f'Home_{s}_Rank' for s in ranking_systems]\n",
    "    df['Home_AvgRank'] = df[rank_cols].mean(axis=1)\n",
    "    df['Home_StdRank'] = df[rank_cols].std(axis=1)  # Disagreement among rankers\n",
    "    \n",
    "    rank_cols = [f'Away_{s}_Rank' for s in ranking_systems]\n",
    "    df['Away_AvgRank'] = df[rank_cols].mean(axis=1)\n",
    "    df['Away_StdRank'] = df[rank_cols].std(axis=1)\n",
    "    \n",
    "    df['AvgRankDiff'] = df['Away_AvgRank'] - df['Home_AvgRank']\n",
    "    \n",
    "    # 8. Strength ratio features\n",
    "    df['Home_BestRank'] = df[[f'Home_{s}_Rank' for s in ranking_systems]].min(axis=1)\n",
    "    df['Away_BestRank'] = df[[f'Away_{s}_Rank' for s in ranking_systems]].min(axis=1)\n",
    "    df['Home_WorstRank'] = df[[f'Home_{s}_Rank' for s in ranking_systems]].max(axis=1)\n",
    "    df['Away_WorstRank'] = df[[f'Away_{s}_Rank' for s in ranking_systems]].max(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "ranking_systems = ['POM']\n",
    "df_model_full = create_comprehensive_features(df_model, ranking_systems)\n",
    "\n",
    "# Step 2: Define feature sets\n",
    "def get_feature_columns(ranking_systems):\n",
    "    \"\"\"\n",
    "    Define all features to use in the model\n",
    "    \"\"\"\n",
    "    feature_cols = ['IsNeutral']\n",
    "    \n",
    "    for system in ranking_systems:\n",
    "        feature_cols.extend([\n",
    "            # Original ranks\n",
    "            f'Home_{system}_Rank',\n",
    "            f'Away_{system}_Rank',\n",
    "            f'{system}_RankDiff',\n",
    "            \n",
    "            # Strength transformations\n",
    "            f'Home_{system}_Strength',\n",
    "            f'Away_{system}_Strength',\n",
    "            f'{system}_StrengthDiff',\n",
    "            \n",
    "            # Log transformations\n",
    "            f'Home_{system}_LogStrength',\n",
    "            f'Away_{system}_LogStrength',\n",
    "            \n",
    "            # Squared strength\n",
    "            f'Home_{system}_Strength2',\n",
    "            f'Away_{system}_Strength2',\n",
    "            \n",
    "            # Top team indicators\n",
    "            f'Home_{system}_IsTop25',\n",
    "            f'Away_{system}_IsTop25',\n",
    "            f'Home_{system}_IsTop50',\n",
    "            f'Away_{system}_IsTop50',\n",
    "            \n",
    "            # Matchup indicators\n",
    "            f'{system}_BothTop25',\n",
    "            f'{system}_BothTop50'\n",
    "        ])\n",
    "    \n",
    "    # Add consensus features\n",
    "    feature_cols.extend([\n",
    "        'Home_AvgRank',\n",
    "        'Away_AvgRank',\n",
    "        'Home_StdRank',\n",
    "        'Away_StdRank',\n",
    "        'AvgRankDiff',\n",
    "        'Home_BestRank',\n",
    "        'Away_BestRank',\n",
    "        'Home_WorstRank',\n",
    "        'Away_WorstRank'\n",
    "    ])\n",
    "    \n",
    "    return feature_cols\n",
    "\n",
    "feature_cols = get_feature_columns(ranking_systems)\n",
    "\n",
    "\n",
    "# Step 3: Prepare train/val/test splits\n",
    "def prepare_data_splits(df_model_full, feature_cols, \n",
    "                       train_seasons=range(2010, 2022),\n",
    "                       val_seasons=[2022, 2023],\n",
    "                       test_season=2024,\n",
    "                       target_day_range=(90, 120)):\n",
    "    \"\"\"\n",
    "    Split data into train, validation, and test sets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training data: historical seasons, all days up to day 120\n",
    "    train_data = df_model_full[\n",
    "        (df_model_full['Season'].isin(train_seasons)) &\n",
    "        (df_model_full['DayNum'] <= 120)\n",
    "    ].copy()\n",
    "    \n",
    "    # Validation data: recent seasons in target day range\n",
    "    val_data = df_model_full[\n",
    "        (df_model_full['Season'].isin(val_seasons)) &\n",
    "        (df_model_full['DayNum'] >= target_day_range[0]) &\n",
    "        (df_model_full['DayNum'] <= target_day_range[1])\n",
    "    ].copy()\n",
    "    \n",
    "    # Test data: most recent season in target day range\n",
    "    test_data = df_model_full[\n",
    "        (df_model_full['Season'] == test_season) &\n",
    "        (df_model_full['DayNum'] >= target_day_range[0]) &\n",
    "        (df_model_full['DayNum'] <= target_day_range[1])\n",
    "    ].copy()\n",
    "    \n",
    "    # Extract X and y\n",
    "    X_train = train_data[feature_cols].copy()\n",
    "    y_train = train_data['HomeSpread'].copy()\n",
    "    \n",
    "    X_val = val_data[feature_cols].copy()\n",
    "    y_val = val_data['HomeSpread'].copy()\n",
    "    \n",
    "    X_test = test_data[feature_cols].copy()\n",
    "    y_test = test_data['HomeSpread'].copy()\n",
    "    \n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = prepare_data_splits(\n",
    "    df_model_full, \n",
    "    feature_cols,\n",
    "    train_seasons=range(2015, 2023),\n",
    "    val_seasons=[2023, 2024],\n",
    "    test_season=2025,\n",
    "    target_day_range=(90, 120)\n",
    ")\n",
    "\n",
    "# Step 4: Define monotonic constraints\n",
    "def get_monotonic_constraints(feature_cols, ranking_systems):\n",
    "    \"\"\"\n",
    "    Define monotonic constraints for ranking features\n",
    "    \n",
    "    Logic:\n",
    "    - Higher HOME rank (worse) → LOWER home spread → constraint = -1\n",
    "    - Higher AWAY rank (worse) → HIGHER home spread → constraint = +1\n",
    "    - Positive RankDiff (away worse) → HIGHER home spread → constraint = +1\n",
    "    - Strength is inverse of rank, so opposite constraints\n",
    "    \"\"\"\n",
    "    \n",
    "    constraints = []\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        if col == 'IsNeutral':\n",
    "            constraints.append(0)  # No constraint\n",
    "        \n",
    "        # Original rank features\n",
    "        elif '_Rank' in col and 'Home_' in col and 'Avg' not in col and 'Best' not in col and 'Worst' not in col:\n",
    "            constraints.append(-1)  # Higher home rank → lower spread\n",
    "        elif '_Rank' in col and 'Away_' in col and 'Avg' not in col and 'Best' not in col and 'Worst' not in col:\n",
    "            constraints.append(1)   # Higher away rank → higher spread\n",
    "        elif 'RankDiff' in col:\n",
    "            constraints.append(1)   # Positive diff (away worse) → higher spread\n",
    "        \n",
    "        # Strength features (inverse of rank)\n",
    "        elif 'Strength' in col and 'Home_' in col and 'Diff' not in col:\n",
    "            constraints.append(1)   # Higher home strength → higher spread\n",
    "        elif 'Strength' in col and 'Away_' in col and 'Diff' not in col:\n",
    "            constraints.append(-1)  # Higher away strength → lower spread\n",
    "        elif 'StrengthDiff' in col:\n",
    "            constraints.append(1)   # Positive diff (home stronger) → higher spread\n",
    "        \n",
    "        # Average/consensus ranks\n",
    "        elif 'Home_AvgRank' in col:\n",
    "            constraints.append(-1)\n",
    "        elif 'Away_AvgRank' in col:\n",
    "            constraints.append(1)\n",
    "        elif 'AvgRankDiff' in col:\n",
    "            constraints.append(1)\n",
    "        \n",
    "        # Best/Worst ranks\n",
    "        elif 'Home_BestRank' in col or 'Home_WorstRank' in col:\n",
    "            constraints.append(-1)\n",
    "        elif 'Away_BestRank' in col or 'Away_WorstRank' in col:\n",
    "            constraints.append(1)\n",
    "        \n",
    "        # No constraint for other features (top25 indicators, std, etc.)\n",
    "        else:\n",
    "            constraints.append(0)\n",
    "    \n",
    "    return tuple(constraints)\n",
    "\n",
    "monotonic_constraints = get_monotonic_constraints(feature_cols, ranking_systems)\n",
    "\n",
    "\n",
    "# Create DMatrix objects\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_cols)\n",
    "dval = xgb.DMatrix(X_val, label=y_val, feature_names=feature_cols)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, feature_names=feature_cols)\n",
    "\n",
    "# XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'mae',\n",
    "    'monotone_constraints': monotonic_constraints,\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.05,  # Learning rate\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 0.1,\n",
    "    'lambda': 1.0,  # L2 regularization\n",
    "    'alpha': 0.1,   # L1 regularization\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "for key, value in params.items():\n",
    "    if key != 'monotone_constraints':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Train model\n",
    "evals = [(dtrain, 'train'), (dval, 'val')]\n",
    "\n",
    "xgb_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "\n",
    "# Step 6: Evaluate model\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, feature_cols):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create DMatrix for predictions\n",
    "    dtrain = xgb.DMatrix(X_train, feature_names=feature_cols)\n",
    "    dval = xgb.DMatrix(X_val, feature_names=feature_cols)\n",
    "    dtest = xgb.DMatrix(X_test, feature_names=feature_cols)\n",
    "    \n",
    "    # Predictions\n",
    "    train_pred = model.predict(dtrain)\n",
    "    val_pred = model.predict(dval)\n",
    "    test_pred = model.predict(dtest)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'Train': {\n",
    "            'MAE': mean_absolute_error(y_train, train_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "            'R2': r2_score(y_train, train_pred),\n",
    "            'N': len(y_train)\n",
    "        },\n",
    "        'Validation': {\n",
    "            'MAE': mean_absolute_error(y_val, val_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_val, val_pred)),\n",
    "            'R2': r2_score(y_val, val_pred),\n",
    "            'N': len(y_val)\n",
    "        },\n",
    "        'Test': {\n",
    "            'MAE': mean_absolute_error(y_test, test_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "            'R2': r2_score(y_test, test_pred),\n",
    "            'N': len(y_test)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(results_df.to_string())\n",
    "    \n",
    "    # Residual analysis\n",
    "\n",
    "    \n",
    "    test_residuals = y_test - test_pred\n",
    "    \n",
    "    print(f\"Mean Error: {test_residuals.mean():.3f}\")\n",
    "    print(f\"Median Absolute Error: {np.median(np.abs(test_residuals)):.3f}\")\n",
    "    print(f\"Std of Errors: {test_residuals.std():.3f}\")\n",
    "    print(f\"\\nError Percentiles:\")\n",
    "    print(f\"  10th: {np.percentile(np.abs(test_residuals), 10):.3f}\")\n",
    "    print(f\"  25th: {np.percentile(np.abs(test_residuals), 25):.3f}\")\n",
    "    print(f\"  50th: {np.percentile(np.abs(test_residuals), 50):.3f}\")\n",
    "    print(f\"  75th: {np.percentile(np.abs(test_residuals), 75):.3f}\")\n",
    "    print(f\"  90th: {np.percentile(np.abs(test_residuals), 90):.3f}\")\n",
    "    \n",
    "    # Within X points accuracy\n",
    "    for threshold in [3, 5, 7, 10]:\n",
    "        within = (np.abs(test_residuals) <= threshold).mean() * 100\n",
    "        print(f\"  {threshold} points: {within:.1f}%\")\n",
    "    \n",
    "    return results_df, train_pred, val_pred, test_pred\n",
    "\n",
    "results_df, train_pred, val_pred, test_pred = evaluate_model(\n",
    "    xgb_model, X_train, y_train, X_val, y_val, X_test, y_test, feature_cols\n",
    ")\n",
    "\n",
    "# Step 7: Feature importance analysis\n",
    "def plot_feature_importance(model, feature_cols, top_n=20):\n",
    "    \"\"\"\n",
    "    Plot feature importance\n",
    "    \"\"\"\n",
    "    \n",
    "    importance = model.get_score(importance_type='gain')\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    importance_df = pd.DataFrame([\n",
    "        {'feature': k, 'importance': v} \n",
    "        for k, v in importance.items()\n",
    "    ]).sort_values('importance', ascending=False)\n",
    "    \n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    top_features = importance_df.head(top_n)\n",
    "    ax.barh(range(len(top_features)), top_features['importance'], color='steelblue', alpha=0.8)\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['feature'])\n",
    "    ax.set_xlabel('Importance (Gain)', fontweight='bold', fontsize=12)\n",
    "    ax.set_title(f'Top {top_n} Most Important Features', fontweight='bold', fontsize=14)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "importance_df = plot_feature_importance(xgb_model, feature_cols, top_n=20)\n",
    "\n",
    "# Step 8: Prediction visualizations\n",
    "def plot_predictions(y_true, y_pred, set_name='Test'):\n",
    "    \"\"\"\n",
    "    Visualize predictions vs actual\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # 1. Scatter plot\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.scatter(y_true, y_pred, alpha=0.5, s=20)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_true.min(), y_pred.min())\n",
    "    max_val = max(y_true.max(), y_pred.max())\n",
    "    ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax1.set_xlabel('Actual Home Spread', fontweight='bold', fontsize=11)\n",
    "    ax1.set_ylabel('Predicted Home Spread', fontweight='bold', fontsize=11)\n",
    "    ax1.set_title(f'{set_name} Set: Predicted vs Actual\\nMAE: {mean_absolute_error(y_true, y_pred):.2f}', \n",
    "                 fontweight='bold', fontsize=12)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Residual plot\n",
    "    ax2 = axes[0, 1]\n",
    "    residuals = y_true - y_pred\n",
    "    ax2.scatter(y_pred, residuals, alpha=0.5, s=20)\n",
    "    ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    ax2.set_xlabel('Predicted Home Spread', fontweight='bold', fontsize=11)\n",
    "    ax2.set_ylabel('Residual (Actual - Predicted)', fontweight='bold', fontsize=11)\n",
    "    ax2.set_title('Residual Plot', fontweight='bold', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax3.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "    ax3.set_xlabel('Residual', fontweight='bold', fontsize=11)\n",
    "    ax3.set_ylabel('Frequency', fontweight='bold', fontsize=11)\n",
    "    ax3.set_title(f'Residual Distribution\\nMean: {residuals.mean():.2f}, Std: {residuals.std():.2f}', \n",
    "                 fontweight='bold', fontsize=12)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Error by prediction magnitude\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Bin predictions\n",
    "    pred_bins = pd.cut(y_pred, bins=10)\n",
    "    error_by_bin = pd.DataFrame({\n",
    "        'pred': y_pred,\n",
    "        'error': np.abs(residuals),\n",
    "        'bin': pred_bins\n",
    "    }).groupby('bin')['error'].mean()\n",
    "    \n",
    "    bin_centers = [interval.mid for interval in error_by_bin.index]\n",
    "    ax4.plot(bin_centers, error_by_bin.values, marker='o', linewidth=2, markersize=8)\n",
    "    ax4.set_xlabel('Predicted Spread (binned)', fontweight='bold', fontsize=11)\n",
    "    ax4.set_ylabel('Mean Absolute Error', fontweight='bold', fontsize=11)\n",
    "    ax4.set_title('Error by Prediction Magnitude', fontweight='bold', fontsize=12)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(y_test, test_pred, 'Test')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
